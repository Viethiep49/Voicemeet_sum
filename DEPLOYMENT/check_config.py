"""
Ki·ªÉm tra v√† hi·ªÉn th·ªã c·∫•u h√¨nh t·ª± ƒë·ªông ph√°t hi·ªán
File: DEPLOYMENT/check_config.py
"""
import sys
import io
from pathlib import Path

# Fix encoding for Windows console
if sys.platform == 'win32':
    try:
        sys.stdout = io.TextIOWrapper(sys.stdout.buffer, encoding='utf-8', errors='replace')
        sys.stderr = io.TextIOWrapper(sys.stderr.buffer, encoding='utf-8', errors='replace')
    except:
        pass

# Add project root to path
sys.path.insert(0, str(Path(__file__).parent.parent))

from config.settings import TRANSCRIPTION, auto_detect_whisper_model, auto_detect_device, auto_detect_compute_type

def main():
    print("=" * 70)
    print("  KI·ªÇM TRA C·∫§U H√åNH T·ª∞ ƒê·ªòNG")
    print("=" * 70)
    print()
    
    # GPU Info
    try:
        import torch
        if torch.cuda.is_available():
            gpu_name = torch.cuda.get_device_name(0)
            vram_gb = torch.cuda.get_device_properties(0).total_memory / (1024**3)
            print(f"üéÆ GPU: {gpu_name}")
            print(f"üíæ VRAM: {vram_gb:.1f} GB")
        else:
            print("‚ö†Ô∏è  GPU: Kh√¥ng c√≥ (s·ª≠ d·ª•ng CPU)")
    except ImportError:
        print("‚ö†Ô∏è  PyTorch ch∆∞a ƒë∆∞·ª£c c√†i ƒë·∫∑t")
    except Exception as e:
        print(f"‚ö†Ô∏è  L·ªói ki·ªÉm tra GPU: {e}")
    
    print()
    print("-" * 70)
    print("üìã C·∫§U H√åNH T·ª∞ ƒê·ªòNG PH√ÅT HI·ªÜN:")
    print("-" * 70)
    print(f"  Model:        {TRANSCRIPTION.model}")
    print(f"  Device:       {TRANSCRIPTION.device}")
    print(f"  Compute Type: {TRANSCRIPTION.compute_type}")
    print(f"  Language:     {TRANSCRIPTION.language}")
    print(f"  Beam Size:    {TRANSCRIPTION.beam_size}")
    print()
    
    # Model recommendations
    print("-" * 70)
    print("üí° TH√îNG TIN MODEL:")
    print("-" * 70)
    model_info = {
        "tiny": "Nhanh nh·∫•t, ƒë·ªô ch√≠nh x√°c th·∫•p (~39M params)",
        "base": "Nhanh, ƒë·ªô ch√≠nh x√°c trung b√¨nh (~74M params)",
        "small": "C√¢n b·∫±ng t·ªëc ƒë·ªô/ch·∫•t l∆∞·ª£ng (~244M params) - KHUY√äN D√ôNG cho RTX 3060",
        "medium": "Ch·∫•t l∆∞·ª£ng t·ªët, ch·∫≠m h∆°n (~769M params) - Cho GPU m·∫°nh h∆°n",
        "large-v2": "Ch·∫•t l∆∞·ª£ng cao nh·∫•t, r·∫•t ch·∫≠m (~1550M params)",
        "large-v3": "Ch·∫•t l∆∞·ª£ng cao nh·∫•t, r·∫•t ch·∫≠m (~1550M params)"
    }
    
    current_model = TRANSCRIPTION.model
    if current_model in model_info:
        print(f"  {current_model}: {model_info[current_model]}")
    print()
    
    # Override instructions
    print("-" * 70)
    print("üîß N·∫æU MU·ªêN THAY ƒê·ªîI MODEL:")
    print("-" * 70)
    print("  Ch·ªânh s·ª≠a file: config/settings.py")
    print("  T√¨m d√≤ng: model: str = None")
    print("  Thay ƒë·ªïi th√†nh: model: str = 'medium'  # ho·∫∑c 'small', 'base', 'tiny'")
    print()
    print("  Ho·∫∑c set tr·ª±c ti·∫øp trong code:")
    print("  from config.settings import TRANSCRIPTION")
    print("  TRANSCRIPTION.model = 'medium'")
    print()
    
    print("=" * 70)

if __name__ == "__main__":
    main()

