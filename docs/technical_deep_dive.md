# ğŸ”§ Technical Deep Dive - Voicemeet_sum

**TÃ i liá»‡u nÃ y chá»©a:** Architecture details, data flow, technical choices, optimizations, challenges

---

## ğŸ—ï¸ SYSTEM ARCHITECTURE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     VOICEMEET_SUM SYSTEM                     â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”           â”‚
â”‚  â”‚   Web UI   â”‚â”€â”€â”€â”€â”€â”€â”€â”€â–¶â”‚   FastAPI Backend    â”‚           â”‚
â”‚  â”‚ (HTML/JS)  â”‚  HTTP   â”‚    (Python 3.10)     â”‚           â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  POST   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜           â”‚
â”‚                              â”‚                               â”‚
â”‚                              â–¼                               â”‚
â”‚                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚                   â”‚  Meeting Pipeline  â”‚                    â”‚
â”‚                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                              â”‚                               â”‚
â”‚        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”‚
â”‚        â–¼                     â–¼                     â–¼        â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”      â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”‚
â”‚  â”‚  FFmpeg  â”‚        â”‚   Whisper   â”‚      â”‚   Qwen   â”‚    â”‚
â”‚  â”‚Processor â”‚â”€â”€â”€â”€â”€â”€â”€â–¶â”‚  Service    â”‚â”€â”€â”€â”€â”€â–¶â”‚ Service  â”‚    â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â”‚
â”‚       â”‚                     â”‚                     â”‚         â”‚
â”‚       â–¼                     â–¼                     â–¼         â”‚
â”‚  Audio.wav          Transcript.txt         Summary.txt     â”‚
â”‚  (16kHz mono)       (95% accuracy)         (Key points)    â”‚
â”‚                                                              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

External Dependencies:
- Ollama (localhost:11434) â†’ Qwen 2.5 LLM
- CUDA Runtime â†’ GPU Acceleration
- FFmpeg binary â†’ Audio processing
```

---

## ğŸ“Š DATA FLOW

### Step 1: Upload
```
User uploads audio.m4a (2GB max)
  â†“
FastAPI validates (extension, size)
  â†“
Save to temp/ with UUID
  â†“
Create job in memory store
  â†“
Return job_id to client
```

### Step 2: Processing (Background Async)
```
Read audio from temp/
  â†“
[FFmpeg Preprocessing - 30-60s]
  - Convert to WAV 16kHz mono
  - Normalize audio levels (-23 LUFS)
  - Remove silence (VAD threshold)
  â†“
[Whisper Transcription - 6-8min for 2hr]
  - Load model to GPU VRAM (5-6GB)
  - Process in 30-second chunks
  - Beam search decoding (beam_size=5)
  - Language detection (auto vi/ja/en)
  - Output: segments with timestamps
  â†“
[Text Processing - 5-10s]
  - Join segments
  - Clean text (remove artifacts)
  - Punctuation restoration
  â†“
[Qwen Summarization - 2-3min]
  - Chunk text (4000 tokens/chunk)
  - Send to Ollama API
  - Structured prompt engineering
  - Extract: summary, action items, decisions
  â†“
[Save Outputs]
  - output/transcript_*.txt
  - output/summary_*.txt
  - Update job status â†’ "completed"
```

### Step 3: Download
```
Client polls /api/status/{job_id}
  â†“
When status="completed"
  â†“
Download via /api/download/{job_id}/{type}
```

---

## ğŸ¯ KEY TECHNICAL CHOICES & RATIONALE

### Q: Táº¡i sao dÃ¹ng Faster-Whisper thay vÃ¬ OpenAI Whisper?

**A: Faster-Whisper faster 3-5x thanks to:**
- CTranslate2 backend (optimized inference engine)
- int8 quantization support
- Better CUDA kernel usage
- Lower memory footprint

**Benchmark (2-hour audio, RTX 4070):**
```
OpenAI Whisper large-v2:  ~25 minutes
Faster-Whisper large-v2:  ~6 minutes
Faster-Whisper medium:    ~4 minutes â† Our choice
```

**Medium model chosen for:**
- Good accuracy/speed tradeoff (92-95% Vietnamese)
- Fits in 6GB VRAM
- Fast enough for production

---

### Q: Táº¡i sao dÃ¹ng Qwen 2.5 thay vÃ¬ GPT?

**Qwen 2.5 advantages:**
- âœ… 100% offline (no API calls, no costs)
- âœ… Native Vietnamese support
- âœ… Open-source (customizable)
- âœ… Good performance (7B ~ GPT-3.5)
- âœ… Runs on consumer GPU (7GB RAM)

**Tradeoffs:**
- âŒ Slightly lower quality than GPT-4
- âŒ Needs local GPU
- âŒ Slower than API

**Decision:** Offline + privacy + cost > slight quality loss

---

### Q: Táº¡i sao dÃ¹ng FastAPI thay vÃ¬ Flask/Django?

**FastAPI advantages:**
- âœ… Async/await native (non-blocking)
- âœ… Auto OpenAPI docs (Swagger UI)
- âœ… Type hints + validation (Pydantic)
- âœ… Modern Python (3.10+)
- âœ… Fast performance
- âœ… WebSocket support (future realtime)

**Use case:**
- Long-running jobs (6-10 min) â†’ async crucial
- Background processing â†’ asyncio.create_task()
- API-first design â†’ auto docs helpful

---

## âš¡ PERFORMANCE OPTIMIZATION TECHNIQUES

### 1. GPU Memory Management

```python
# Problem: Whisper model uses 5-6GB VRAM
# Solution: Unload model after transcription

class WhisperService:
    def transcribe(self, audio):
        self.model = WhisperModel(
            model_size="medium",
            device="cuda",
            compute_type="float16"  # Half precision â†’ 50% memory
        )
        result = self.model.transcribe(audio)

        # Critical: Free GPU memory
        del self.model
        torch.cuda.empty_cache()

        return result
```

---

### 2. Audio Preprocessing Optimization

```bash
# FFmpeg one-pass processing
ffmpeg -i input.m4a \
  -ar 16000 \           # Resample to 16kHz (Whisper requirement)
  -ac 1 \               # Convert to mono (50% size reduction)
  -af "loudnorm=I=-23:LRA=7:TP=-2.0" \  # Normalize loudness
  -af "silenceremove=start_periods=1:start_silence=0.1" \  # Remove silence
  output.wav

# Result: Better accuracy + faster processing
```

---

### 3. Chunked Summarization

```python
# Problem: Long transcripts (20k+ tokens) exceed LLM context
# Solution: Chunking + hierarchical summarization

def summarize_long_text(text, chunk_size=4000):
    chunks = split_into_chunks(text, chunk_size)

    # Step 1: Summarize each chunk
    summaries = [summarize_chunk(chunk) for chunk in chunks]

    # Step 2: Combine summaries
    combined = "\n\n".join(summaries)

    # Step 3: Final summary of summaries
    final = summarize_chunk(combined)

    return final
```

---

## ğŸš§ CHALLENGES & SOLUTIONS

### Challenge 1: Mixed Language Transcription

**Problem:** Whisper struggles with code-switching (Viá»‡t â†” Nháº­t)

**Solution:**
1. Use `language="vi"` as primary
2. Add `initial_prompt` with mixed vocab:
   ```
   "Cuá»™c há»p cÃ´ng ty F&B, cÃ³ tá»« tiáº¿ng Nháº­t vá» thá»±c pháº©m
    nhÆ° phá»Ÿ, bÃºn, ramen (ãƒ©ãƒ¼ãƒ¡ãƒ³), sushi (å¯¿å¸)"
   ```
3. Post-process: Detect Japanese tokens, re-transcribe segments

**Result:** Accuracy improved from 75% â†’ 88% on mixed audio

---

### Challenge 2: Accurate Punctuation

**Problem:** Whisper generates unpunctuated text

**Solution:**
1. Enable `word_timestamps=True`
2. Analyze pauses between words
3. Insert punctuation based on:
   - Pause length (>0.5s â†’ period, >0.3s â†’ comma)
   - Sentence length (>15 words â†’ likely end)
4. LLM post-processing for final cleanup

**Result:** Readable paragraphs instead of wall of text

---

### Challenge 3: Action Items Extraction

**Problem:** Generic summary misses actionable tasks

**Solution:** Custom prompt engineering:
```
Analyze this meeting transcript and extract:

1. DECISIONS MADE (cÃ¡c quyáº¿t Ä‘á»‹nh)

2. ACTION ITEMS (cÃ´ng viá»‡c cáº§n lÃ m):
   - Task description
   - Assignee (if mentioned)
   - Deadline (if mentioned)

3. FOLLOW-UP TOPICS (váº¥n Ä‘á» cáº§n tháº£o luáº­n thÃªm)

Format as structured markdown.
```

**Result:** Actionable output, ready for project management tools

---

## ğŸ“ˆ PERFORMANCE METRICS

### Processing Speed Breakdown
```
2-hour audio file processing:

Total: 9-12 minutes (10-13x realtime)
â”œâ”€ FFmpeg:     30-60 seconds (5-8%)
â”œâ”€ Whisper:    6-8 minutes (67-75%)
â””â”€ Qwen:       2-3 minutes (17-25%)
```

### Accuracy Metrics
```
Word Error Rate (WER):

Vietnamese:
â”œâ”€ Clean audio:        95% accuracy (5% WER)
â”œâ”€ Normal (some noise): 92% accuracy (8% WER)
â””â”€ Poor quality:       85% accuracy (15% WER)

Japanese:
â”œâ”€ Clean:   90% accuracy
â”œâ”€ Normal:  88% accuracy
â””â”€ Poor:    80% accuracy

English:
â”œâ”€ Clean:   96% accuracy
â”œâ”€ Normal:  94% accuracy
â””â”€ Poor:    88% accuracy
```

### Resource Usage
```
GPU:
â”œâ”€ Whisper VRAM: 5-6GB (during transcription)
â”œâ”€ Qwen VRAM:    7GB (during summarization)
â””â”€ Peak total:   ~8GB (models don't overlap)

CPU:
â””â”€ ~40% utilization (mostly idle during GPU processing)

Disk:
â””â”€ 2GB temp storage per job (auto-cleanup)
```

---

## ğŸ” SECURITY & PRIVACY

### Data Handling
- âœ… 100% local processing (no cloud uploads)
- âœ… Automatic temp file cleanup after processing
- âœ… No telemetry, no external API calls (except localhost Ollama)
- âœ… Can run on air-gapped networks

### For Enterprise
- Optional encryption at rest
- Role-based access control (roadmap)
- Audit logs (roadmap)
- GDPR/HIPAA compliance ready

---

## ğŸ”„ SCALABILITY CONSIDERATIONS

### Current (Single GPU)
```
Throughput:
â”œâ”€ Sequential: 6-7 jobs/hour (10min each)
â”œâ”€ With queue: ~50 jobs/day
â””â”€ Max concurrent: 1 job at a time (GPU limitation)
```

### Future Scaling Options
```
Option 1: Multiple GPUs
- Linear scaling (2 GPUs = 2x throughput)
- Requires GPU queue management

Option 2: Distributed System
- Worker pool with job queue (Celery/RQ)
- Redis for job state
- PostgreSQL for persistence

Option 3: Cloud Deployment
- Docker containers
- Kubernetes orchestration
- Auto-scaling based on queue length
```

---

## ğŸ› ï¸ DEVELOPMENT BEST PRACTICES

### Code Organization
```
voicemeet_sum/
â”œâ”€ app/
â”‚  â”œâ”€ main.py              # FastAPI app
â”‚  â”œâ”€ services/
â”‚  â”‚  â”œâ”€ ffmpeg_service.py
â”‚  â”‚  â”œâ”€ whisper_service.py
â”‚  â”‚  â””â”€ qwen_service.py
â”‚  â”œâ”€ models/              # Pydantic models
â”‚  â””â”€ utils/               # Helper functions
â”œâ”€ static/                 # Web UI
â”œâ”€ temp/                   # Temp uploads
â”œâ”€ output/                 # Results
â””â”€ tests/                  # Unit tests
```

### Error Handling
```python
try:
    result = await process_audio(file_path)
except FFmpegError as e:
    logger.error(f"FFmpeg failed: {e}")
    update_job_status(job_id, "failed", error=str(e))
except WhisperError as e:
    logger.error(f"Whisper failed: {e}")
    # Retry logic here
except Exception as e:
    logger.exception("Unexpected error")
    # Graceful degradation
```

### Logging
```python
import logging

logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('app.log'),
        logging.StreamHandler()
    ]
)
```

---

## ğŸ“Š MONITORING & METRICS

### Key Metrics to Track
```
Processing Metrics:
- Jobs processed/hour
- Average processing time
- Error rate by stage (FFmpeg/Whisper/Qwen)
- Queue length

Resource Metrics:
- GPU utilization %
- GPU memory usage
- CPU usage
- Disk space

Quality Metrics:
- Average confidence score (Whisper)
- User corrections rate
- Summary quality feedback
```

### Health Checks
```python
@app.get("/health")
async def health_check():
    return {
        "status": "healthy",
        "ollama": check_ollama_connection(),
        "gpu": check_gpu_availability(),
        "disk_space": check_disk_space()
    }
```

---

## ğŸ”® FUTURE TECHNICAL IMPROVEMENTS

### Phase 2 (In Progress)
- **Speaker Diarization**: pyannote-audio integration
- **Better UI**: Real-time progress, live preview
- **Export Formats**: PDF, DOCX, JSON
- **Confidence Scoring**: Highlight uncertain parts

### Phase 3
- **Realtime Transcription**: WebSocket streaming
- **Custom Vocabulary**: Domain-specific terms
- **Fine-tuning**: Custom models per industry
- **Active Learning**: Learn from user corrections

### Phase 4
- **Multi-GPU Support**: Parallel processing
- **Distributed Architecture**: Microservices
- **Advanced Analytics**: Speaker insights, sentiment
- **API Platform**: Developer integrations

---

**Document Version:** 1.0
**Last Updated:** 2025-12-17
